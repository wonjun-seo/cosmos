{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc3adc7b",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea8c3f6",
   "metadata": {},
   "source": [
    "## Recap\n",
    "Let $f: \\mathbb{R} \\to \\mathbb{R}$ be our **target function** to be minimized (or maximized). This can be written as\n",
    "$$ x^\\ast = \\argmin_{x \\in \\mathbb{R}} f(x). $$\n",
    "$x^\\ast$ is called a **global minimum** of $f$.\n",
    "\n",
    "The gradient descent (GD) is an iterative algorithm to find $x^\\ast$. The vanilla GD requires the following:\n",
    "- $x_0$: initialization point\n",
    "- $\\eta$: learning rate\n",
    "- Stopping condition\n",
    "\n",
    "The **pseudocode** for GD is as follows:\n",
    "1. **Initialize**: $x \\leftarrow x_0$  \n",
    "2. **Repeat until stopping condition is met**:\n",
    "   $$x \\leftarrow x - \\eta \\cdot f'(x)$$\n",
    "3. **Return**: $x$\n",
    "\n",
    "**Examples of stopping conditions**\n",
    "- Number of iterations\n",
    "- $|f(x_n) - f(x_{n-1})| < \\epsilon$ for a very small $\\epsilon$\n",
    "- $|x_n - x_{n-1}| < \\epsilon$ for a very small $\\epsilon$\n",
    "- $|f'(x_n)| < \\epsilon$ for a very small $\\epsilon$\n",
    "\n",
    "Let's see how it works with a quadratic function $y = ax^2 + bx + c$ with $a > 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35021a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Coefficients of f\n",
    "a = 3 # should be positive\n",
    "b = 4\n",
    "c = 5\n",
    "\n",
    "def f(x):\n",
    "    return a*x**2 + b*x + c\n",
    "\n",
    "def der_f(x):\n",
    "    return 2*a*x + b\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 0.1                # learning rate eta\n",
    "N = 10                  # Stopping condition: 10 iterations\n",
    "\n",
    "# Initialization\n",
    "np.random.seed(87)\n",
    "x = np.random.randn()   # random number from standard normal distribution\n",
    "\n",
    "# History\n",
    "x_history = np.array([x])\n",
    "\n",
    "# GD\n",
    "for _ in range(N):\n",
    "    der = der_f(x)\n",
    "    x = x - lr * der    # updating\n",
    "    x_history = np.append(x_history, x)\n",
    "\n",
    "print(f\"Minimizer of x: {x_history[-1]:.3f}\")\n",
    "\n",
    "y_history = np.array([f(x) for x in x_history])\n",
    "\n",
    "x_min, x_max = x_history.min(), x_history.max()\n",
    "padding = (x_max - x_min)*0.3\n",
    "x_points = np.linspace(x_min - padding, x_max + padding, 100)\n",
    "y_points = np.array([f(x) for x in x_points])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.plot(x_points, y_points, label=r\"$f(x) = (x - 3)^2$\", color=\"blue\")\n",
    "ax.plot(x_history, y_history, 'o-r', label=r\"x_n\")\n",
    "for i, (xi, yi) in enumerate(zip(x_history, y_history)):\n",
    "    ax.text(xi, yi + 0.3, str(i), ha='center', fontsize=8, color='black')\n",
    "    \n",
    "ax.set_title(\"Gradient Descent on quadratic function\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"f(x)\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991199c1",
   "metadata": {},
   "source": [
    "#### Discussion: Limitation\n",
    "What conditions must the function $f$ satisfy to ensure that vanilla gradient descent converges to a **global minimum**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5f2613",
   "metadata": {},
   "source": [
    "# Gradient Descent in ML\n",
    "## Simple linear regression\n",
    "**Setup**\n",
    "1. Model\n",
    "$$ y = b + w x + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0,1). $$\n",
    "2. Data: $\\{x_i, y_i\\}$ for $i = 1, \\dots, n$.\n",
    "3. Goal: What is the best $(b, \\ w)$ that explains the data based on the model.\n",
    "\n",
    "**Mathematical Formulation**\n",
    "\n",
    "Want to find \n",
    "$$ \\argmin_{(b, \\ w) \\in \\mathbb{R}^2} \\frac{1}{n} \\sum_{i=1}^{n} (b + w x_i - y_i)^2. $$\n",
    "\n",
    "Target function is a bivariate function of $b$ and $w$.\n",
    "$$ L(b, w) = \\frac{1}{n} \\sum_{i=1}^{n} (b + w x_i - y_i)^2. $$\n",
    "Note that $x_i$ and $y_i$ are constants (given data) in $L$. Even though $L$ is bivariate, the logic is the same. \n",
    "\n",
    "We update each variable by subtracting (learning rate) $\\times$ (derivative)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6680bc5f",
   "metadata": {},
   "source": [
    "**Partial Derivatives**\n",
    "$$ \\begin{gathered}\n",
    "\\frac{\\partial L}{\\partial b} (b, w) = \\frac{2}{n} \\sum_{i=1}^{n} (b + w x_i - y_i), \\\\\n",
    "\\frac{\\partial L}{\\partial w} (b, w) = \\frac{2}{n} \\sum_{i=1}^{n} x_i (b + w x_i - y_i).\n",
    "\\end{gathered} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacdee7b",
   "metadata": {},
   "source": [
    "**Pseudocode**\n",
    "Let $(b_0, w_0)$ be the initial point and $\\eta$ be the learning rate. For pre-specified stopping condition, the pseudocode of GD is as follows:\n",
    "1. **Initialize**: $b \\leftarrow b_0$ and $w \\leftarrow w_0$  \n",
    "2. **Repeat until stopping condition is met**:\n",
    "   $$ b \\leftarrow b - \\eta \\cdot \\frac{\\partial L}{\\partial b} (b, w)$$\n",
    "   $$ w \\leftarrow w - \\eta \\cdot \\frac{\\partial L}{\\partial w} (b, w)$$\n",
    "3. **Return**: $b$ and $w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0fd9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "np.random.seed(42)\n",
    "N = 100                                             # number of data\n",
    "x = np.linspace(0, 10, N)\n",
    "true_b = 1.0\n",
    "true_w = 2.5\n",
    "y = true_b + true_w * x + np.random.randn(N)        # model\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 0.01\n",
    "n_iters = 5                                      # stopping condition\n",
    "record_gap = 1\n",
    "record = True\n",
    "\n",
    "# Initialization\n",
    "b = 0\n",
    "w = 0\n",
    "\n",
    "# History\n",
    "b_history = np.array([b])\n",
    "w_history = np.array([w])\n",
    "\n",
    "# GD\n",
    "for i in range(n_iters):\n",
    "    y_pred = w * x + b\n",
    "    error = y_pred - y\n",
    "\n",
    "    der_b = 2 * np.mean(error)\n",
    "    der_w = 2 * np.mean(error * x)\n",
    "\n",
    "    b = b - lr * der_b\n",
    "    w = w - lr * der_w\n",
    "    loss = np.mean(error ** 2)\n",
    "\n",
    "    b_history = np.append(b_history, b)\n",
    "    w_history = np.append(w_history, w)\n",
    "\n",
    "    if record:\n",
    "        if i % record_gap == 0:\n",
    "            print(f\"{'Step ' + str(i):<25}: b = {b:.4f}, w = {w:.4f}, target function = {loss:.4f}\")\n",
    "\n",
    "print(f\"{'Final':<25}: b = {b:.4f}, w = {w:.4f}, target function = {loss:.4f}\")\n",
    "\n",
    "# Closed-form solution\n",
    "X = np.vstack([x, np.ones_like(x)]).T\n",
    "w_soln, b_soln = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "print(f\"{'Closed-form solution':<25}: b = {b_soln:.4f}, w = {w_soln:.4f}\")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize = (8,6))\n",
    "ax.scatter(x, y, s=10,label=\"Data\", color=\"black\")\n",
    "if record:\n",
    "    for i in range(n_iters//record_gap):\n",
    "        ax.plot(x, w_history[i*record_gap] * x + b_history[i*record_gap], label=f\"{i*record_gap}th iteration\")\n",
    "else:\n",
    "    ax.plot(x, w * x + b, label=f\"Final\")\n",
    "\n",
    "ax.plot(x, w_soln * x + b_soln, label=f\"Solution\")\n",
    "ax.legend()\n",
    "ax.set_title(\"Simple Linear Regression using Gradient Descent\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833bb1c8",
   "metadata": {},
   "source": [
    "#### Gradient\n",
    "For $L$, We define\n",
    "$$ \\nabla L = \\begin{bmatrix}\n",
    "\\frac{\\partial L}{\\partial b} \\\\\n",
    "\\\\\n",
    "\\frac{\\partial L}{\\partial w}\n",
    "\\end{bmatrix} $$\n",
    "With matrix form, the updating rule of GD can be written as\n",
    "$$ \\begin{bmatrix}\n",
    "b \\\\\n",
    "w\n",
    "\\end{bmatrix} \\leftarrow \\begin{bmatrix}\n",
    "b \\\\\n",
    "w\n",
    "\\end{bmatrix} - \\eta \\nabla L(b, w) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ee1d9c",
   "metadata": {},
   "source": [
    "One can naturally extend simple linear regression to multiple linear regression with notion of gradient:\n",
    "$$ y = b + w_1 x_1 + w_2 x_2 + \\dots + w_d x_d + \\epsilon $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cosmos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
