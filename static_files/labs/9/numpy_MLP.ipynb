{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b760f74",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2688d831",
   "metadata": {},
   "source": [
    "In this notebook, we will construct a multi-layer perceptron (MLP) using `numpy` only. We consider the data sampled from the following model\n",
    "$$ y = \\sin x + 0.1 \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, 1) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd71dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a520876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generation\n",
    "np.random.seed(87)\n",
    "N = 200                                             # number of data\n",
    "X = np.linspace(-np.pi, np.pi, N).reshape(-1, 1)    # For the matrix operation. X: N x 1\n",
    "y = np.sin(X) + 0.1 * np.random.randn(N, 1)         # For the matrix operation. Y: N x 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216d9a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "ax.scatter(X, y, s=5, label=\"data\", color='black')\n",
    "\n",
    "ax.set_title(\"Data\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecc88fb",
   "metadata": {},
   "source": [
    "## 1. Linear regression\n",
    "Obviously, fitting the given data with linear regression will give poor results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71435505",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "epochs = 1000         # number of iterations\n",
    "\n",
    "w = np.zeros((1, 1))  # For the matrix operation\n",
    "b = np.zeros(1)\n",
    "\n",
    "lin_loss_history = np.zeros(epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    y_pred_lin = X @ w + b\n",
    "    error_lin = y_pred_lin - y\n",
    "\n",
    "    lin_loss_history[epoch] = np.mean(error_lin**2)\n",
    "\n",
    "    grad_w = 2 * X.T @ error_lin / len(X)\n",
    "    grad_b = 2 * np.mean(error_lin)\n",
    "\n",
    "    w -= lr * grad_w.T\n",
    "    b -= lr * grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84473e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "ax.scatter(X, y, s=5, label=\"data\", color='black')\n",
    "ax.plot(X, y_pred_lin, label=\"predicted\", color='red')\n",
    "\n",
    "ax.set_title(\"Linear Regression\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ac6135",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lin_loss_history)\n",
    "plt.ylim([0, 0.6])\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Loss history (Linear regression)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068760e8",
   "metadata": {},
   "source": [
    "## 2. 1 Hidden layer with ReLU\n",
    "Recall that \n",
    "$$ \\text{ReLU}(x) = \\max (0, x) = \\begin{cases}\n",
    "x, & x > 0, \\\\\n",
    "0, & x \\le 0.\n",
    "\\end{cases} $$\n",
    "Even though this function is not differentiable at $x = 0$, we define\n",
    "$$ \\text{ReLU}'(x) = \\begin{cases}\n",
    "1, & x > 0, \\\\\n",
    "0, & x \\le 0.\n",
    "\\end{cases} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50781b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "def relu_deriv(x):\n",
    "    return (x > 0).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992a3608",
   "metadata": {},
   "source": [
    "We want to include 1 hidden layer of dimension 10 between input layer and output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749c10f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparmeters\n",
    "input_dim = 1\n",
    "hidden_dim = 5\n",
    "output_dim = 1\n",
    "lr = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "\n",
    "# Initialization\n",
    "np.random.seed(100)\n",
    "W1 = np.random.randn(input_dim, hidden_dim) * 0.1\n",
    "b1 = np.zeros((1, hidden_dim))\n",
    "W2 = np.random.randn(hidden_dim, output_dim) * 0.1\n",
    "b2 = np.zeros((1, output_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d93277",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history_1 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward\n",
    "    h1 = X @ W1 + b1\n",
    "    z1 = relu(h1) \n",
    "    y_pred_1 = z1 @ W2 + b2\n",
    "\n",
    "    # Loss (MSE)\n",
    "    error_1 = y_pred_1 - y\n",
    "    loss_1 = np.mean(error_1**2)\n",
    "    loss_history_1.append(loss_1)\n",
    "\n",
    "    # Backward\n",
    "    dy = 2 * error_1 / len(X)\n",
    "    \n",
    "    dW2 = z1.T @ dy\n",
    "    db2 = np.sum(dy, axis=0, keepdims=True)\n",
    "\n",
    "    dz1 = dy @ W2.T\n",
    "    dh1 = dz1 * relu_deriv(h1)\n",
    "    dW1 = X.T @ dh1\n",
    "    db1 = np.sum(dh1, axis=0, keepdims=True)\n",
    "\n",
    "    # Update\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1\n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: MSE = {loss_1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36241196",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "ax.scatter(X, y, s=5, label=\"data\", color='black')\n",
    "ax.plot(X, y_pred_1, label=\"predicted\", color='red')\n",
    "\n",
    "ax.set_title(f\"1 Hidden Layer (after {epochs} epochs)\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a229257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history_1)\n",
    "plt.ylim([0, 0.6])\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Loss history (1 Hidden Layer)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef9dd66",
   "metadata": {},
   "source": [
    "## 3. 2 Hidden layers with tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27c3ad6",
   "metadata": {},
   "source": [
    "tanh is another popular actvation function. It's defined as\n",
    "$$ \\tanh (x) = \\frac{e^x -e^{-x}}{e^x + e^{-x}}. $$\n",
    "It's derivative is given by\n",
    "$$ \\tanh'(x) = 1 - \\tanh^2(x). $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714dc070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def tanh_grad(z):\n",
    "    return 1 - np.tanh(z) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6750a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparmeters\n",
    "input_dim = 1\n",
    "h1_dim = 50\n",
    "h2_dim = 50\n",
    "output_dim = 1\n",
    "lr = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "# Initialization\n",
    "W1 = np.random.randn(input_dim, h1_dim) * 0.1\n",
    "b1 = np.zeros((1, h1_dim))\n",
    "\n",
    "W2 = np.random.randn(h1_dim, h2_dim) * 0.1\n",
    "b2 = np.zeros((1, h2_dim))\n",
    "\n",
    "W3 = np.random.randn(h2_dim, output_dim) * 0.1\n",
    "b3 = np.zeros((1, output_dim))\n",
    "\n",
    "loss_history_2 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward\n",
    "    h1 = X @ W1 + b1\n",
    "    z1 = tanh(h1)\n",
    "\n",
    "    h2 = z1 @ W2 + b2\n",
    "    z2 = tanh(h2)\n",
    "\n",
    "    y_pred_2 = z2 @ W3 + b3\n",
    "\n",
    "    # Loss\n",
    "    error_2 = y_pred_2 - y\n",
    "    loss_2 = np.mean(error_2 ** 2)\n",
    "    loss_history_2.append(loss_2)\n",
    "\n",
    "    # Backward\n",
    "    dy = 2 * error_2 / len(X)\n",
    "\n",
    "    dW3 = z2.T @ dy\n",
    "    db3 = np.sum(dy, axis=0)\n",
    "\n",
    "    dz2 = dy @ W3.T\n",
    "    dh2 = dz2 * tanh_grad(h2)\n",
    "\n",
    "    dW2 = z1.T @ dh2\n",
    "    db2 = np.sum(dh2, axis=0)\n",
    "\n",
    "    dz1 = dh2 @ W2.T\n",
    "    dh1 = dz1 * tanh_grad(h1)\n",
    "\n",
    "    dW1 = X.T @ dh1\n",
    "    db1 = np.sum(dh1, axis=0)\n",
    "\n",
    "    # Update\n",
    "    W3 -= lr * dW3\n",
    "    b3 -= lr * db3\n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: MSE = {loss_2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cedd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "ax.scatter(X, y, s=5, label=\"data\", color='black')\n",
    "ax.plot(X, y_pred_2, label=\"predicted\", color='red')\n",
    "\n",
    "ax.set_title(f\"2 Hidden Layer (after {epochs} epochs)\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18717c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history_2)\n",
    "plt.ylim([0, 0.6])\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Loss history (3 Hidden layers)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf87fe0",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "In this example, the MLP was trained using the entire dataset. As a result, the loss continues to decrease with more training epochsâ€”but this does not guarantee better performance on new, unseen data. In fact, it may lead to **overfitting**.\n",
    "\n",
    "To achieve better generalization, how could you redesign the training process?\n",
    "\n",
    "- How would you split the dataset?\n",
    "- What would be an appropriate stopping condition to prevent overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5782dd98",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cosmos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
