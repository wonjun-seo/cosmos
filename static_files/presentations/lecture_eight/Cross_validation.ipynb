{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4db977f",
   "metadata": {},
   "source": [
    "# Cross-Validation (CV)\n",
    "\n",
    "### What Is Cross-Validation?\n",
    "\n",
    "Cross-validation (CV) is a resampling technique that repeatedly rotates a subset of the dataset to be the test set. Compared to a split-sample validation, CV lets every observation play roles in both training and testing across different rounds.\n",
    "\n",
    "---\n",
    "\n",
    "### How It Works\n",
    "\n",
    "Consider a K-fold cross-validation ($K = 5, 10$ usually). \n",
    "1. Create K folds by partitioning the data into K equal-size, non-overlapping folds.  \n",
    "2. From $i=1$ to $K$, use the $ith$ fold to evaluate the model trained on the other $K − 1$ folds, \n",
    "3. Average the scores from the $K$ iterations in Step 2\n",
    "4. Select & refit – Pick the model/hyper-parameters with the best average score, then retrain that model on the full dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### Why It Works \n",
    "\n",
    "1. Independent test set in each iteration.    \n",
    "2. Theory shows that iterating through folds has minimum impacts \n",
    "\n",
    "### Pros & Cons\n",
    "\n",
    "**Pros**\n",
    "\n",
    "✅ Use all data for training and testing.  \n",
    "✅ Theoretical guarantee from statistical theory     \n",
    "✅ Simple idea that is easy to adapt to other data types.   \n",
    "\n",
    "**Cons**\n",
    "\n",
    "❌ Computationally expansive (need to run $K$ times for every model and every choice of tuning parameter).  \n",
    "❌ Data leakage risk if not handled property for dependent data.  \n",
    "❌ Does not work for small sample size    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "324d5ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, matplotlib.pyplot as plt, seaborn as sns\n",
    "from ipywidgets import Button, Output, VBox\n",
    "from IPython.display import display\n",
    "from matplotlib.patches import Rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c67391b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79c5ab4a70ec4284a2bbc8797db1aed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Button(button_style='info', description='Randomize folds', layout=Layout(width='160px'), style=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# fixed true function on design grid\n",
    "n = 100\n",
    "x_grid = np.linspace(0, 4, n)\n",
    "f_true = np.sin(np.pi*x_grid) + 0.5*np.sin(2*np.pi*x_grid)\n",
    "sigma2 = 1.0\n",
    "rng    = np.random.default_rng(42)\n",
    "\n",
    "# one noisy sample we’ll keep for all CV runs\n",
    "y_observed = f_true + rng.normal(scale=np.sqrt(sigma2), size=n)\n",
    "\n",
    "def poly_design(x, K):\n",
    "    return np.column_stack([x**k for k in range(1, K+1)])\n",
    "\n",
    "# theoretical total risk for reference\n",
    "def theo_total(K):\n",
    "    X = poly_design(x_grid, K)\n",
    "    H = X @ np.linalg.pinv(X.T @ X) @ X.T\n",
    "    bias2 = np.mean(((np.eye(n)-H) @ f_true)**2)\n",
    "    var   = sigma2 * np.trace(H@H.T)/n\n",
    "    return bias2 + var\n",
    "\n",
    "Kmax = 20\n",
    "Ks   = np.arange(1, Kmax+1)\n",
    "tot_theo = np.array([theo_total(k) for k in Ks])\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# widget components\n",
    "out = Output()\n",
    "rand_btn = Button(description=\"Randomize folds\",\n",
    "                  button_style=\"info\", layout={'width':'160px'})\n",
    "display(VBox([rand_btn, out]))\n",
    "\n",
    "palette = sns.color_palette(\"husl\", 5)  # one colour per fold\n",
    "\n",
    "def redraw(*_):\n",
    "    # ----- random 5-fold split ------------------------------------\n",
    "    perm = rng.permutation(n)\n",
    "    fold_sizes = np.full(5, n//5)\n",
    "    fold_sizes[:n % 5] += 1\n",
    "    folds = np.split(perm, np.cumsum(fold_sizes)[:-1])  # list of 5 index arrays\n",
    "\n",
    "    # empirical risk arrays (5 folds × Kmax)\n",
    "    emp_risk = np.zeros((5, Kmax))\n",
    "\n",
    "    for fold_id, test_idx in enumerate(folds):\n",
    "        train_idx = np.setdiff1d(np.arange(n), test_idx)\n",
    "\n",
    "        for k_idx, K in enumerate(Ks):\n",
    "            # design matrices\n",
    "            X_tr = poly_design(x_grid[train_idx], K)\n",
    "            X_te = poly_design(x_grid[test_idx],  K)\n",
    "\n",
    "            beta  = np.linalg.pinv(X_tr.T @ X_tr) @ X_tr.T @ y_observed[train_idx]\n",
    "            y_hat = X_te @ beta\n",
    "            emp_risk[fold_id, k_idx] = np.mean((y_observed[test_idx] - y_hat)**2)\n",
    "\n",
    "    cv_mean = emp_risk.mean(axis=0)\n",
    "\n",
    "    # ------------------ plotting ----------------------------------\n",
    "    with out:\n",
    "        out.clear_output(wait=True)\n",
    "        fig, (ax_bar, ax_risk) = plt.subplots(\n",
    "            1, 2, figsize=(12, 5), gridspec_kw={'width_ratios':[1.2, 2]})\n",
    "\n",
    "        # ---- left: fold layout bar plot --------------------------\n",
    "                # ---- left: fold composition (100 tiny bars per row) -------\n",
    "        ax_bar.clear()                             # in case of redraw\n",
    "        bar_height   = 0.8                         # a bit of vertical gap\n",
    "        y_positions  = np.linspace(4, 0, 5, endpoint=False)\n",
    "\n",
    "        for row, (y0, test_idx) in enumerate(zip(y_positions, folds)):\n",
    "            test_set = set(test_idx)               # O(1) membership check\n",
    "            for i in range(n):\n",
    "                ax_bar.add_patch(Rectangle((i, y0),      # (x, y)\n",
    "                                            1, bar_height,\n",
    "                                            facecolor=palette[row] if i in test_set\n",
    "                                                     else \"lightgrey\",\n",
    "                                            edgecolor=None))  # ← no boundary\n",
    "\n",
    "        ax_bar.set_xlim(0, n)\n",
    "        ax_bar.set_ylim(-0.2, 5)                   # leave a tad of margin\n",
    "        ax_bar.set_facecolor(\"white\")\n",
    "        ax_bar.set_xticks([]); ax_bar.set_yticks([])\n",
    "        ax_bar.set_title(\"5-fold CV composition\\n(coloured = test fold)\")\n",
    "\n",
    "\n",
    "        # ---- right: risk curves ----------------------------------\n",
    "        ax_risk.plot(Ks, tot_theo, color=\"black\", label=\"theoretical risk\")\n",
    "        for f in range(5):\n",
    "            ax_risk.plot(Ks, emp_risk[f], color=palette[f], alpha=.8,\n",
    "                         label=f\"fold {f+1}\")\n",
    "        ax_risk.plot(Ks, cv_mean, color=\"red\", lw=2.5,\n",
    "                     label=\"CV average\")\n",
    "        best_k = Ks[np.argmin(cv_mean)]\n",
    "        ax_risk.axvline(best_k, ls=\"--\", color=\"red\",\n",
    "                        label=f\"min CV risk (K={best_k})\")\n",
    "        ax_risk.set(xlabel=\"Polynomial degree K\",\n",
    "                    ylabel=\"Empirical / theoretical MSE\",\n",
    "                    title=\"Risk vs model complexity (5-fold CV)\")\n",
    "        ax_risk.legend(); ax_risk.grid(alpha=.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        display(fig); plt.close(fig)\n",
    "\n",
    "# wire button\n",
    "rand_btn.on_click(redraw)\n",
    "redraw()        # initial display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd05a78",
   "metadata": {},
   "source": [
    "Check out this example by [MLU](https://mlu-explain.github.io/cross-validation/). Notice that they actually leave out an additional test set for the grand final comparison. It is not something you have to do in model evaluation. But this is a common practice in competitions to ensure honest evaluation of the model performance. \n",
    "\n",
    "> Note: Even if the test data is hidden from the participants, one can still tune models based on the metrics returned from the test data..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7a336a",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Perform a 5-fold cross-validation on the following dataset to select the best ridge penalty, assuming we fit a linear regression of $y$ on $x_1$, $x_2$, and the intercept term. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ce35e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng(42)        # reproducible\n",
    "n = 50                                # number of observations\n",
    "\n",
    "# design matrix: each row is (x1, x2)\n",
    "X = rng.normal(size=(n, 2))           # shape (50, 2)\n",
    "# prepend a column of 1s for the intercept term\n",
    "X = np.hstack((np.ones((n, 1)), X))   # shape (50, 3)\n",
    "\n",
    "# noise term\n",
    "epsilon = rng.normal(scale=0.5, size=n)\n",
    "\n",
    "y = X[:,0]+X[:, 1] + X[:, 2]**2 + epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18784f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.4675\n"
     ]
    }
   ],
   "source": [
    "# You can start with one simple ridge regression model\n",
    "# Fit a ridge regression model\n",
    "# from sklearn.linear_model import Ridge\n",
    "# ridge = Ridge(alpha=1.0)  # alpha is the penalty that we need to tune\n",
    "\n",
    "# Now to create the folds for cross-validation, we can use KFold from sklearn\n",
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "# Apply f-Fold Cross-Validation on ridge regression to tune the alpha parameter\n",
    "\n",
    "# Visualize the cross-validation results\n",
    "\n",
    "# Select the best alpha and retrain the model on the entire dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c6aa16",
   "metadata": {},
   "source": [
    "### Appendix: variants of cross-validation\n",
    "\n",
    "Not required! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bfbe51",
   "metadata": {},
   "source": [
    "\n",
    "| Variant                                       | Typical syntax in `sklearn`                             | When to use                                                               | Key idea / mechanics                                                             | Pros                                               | Cons / warnings                                                  |\n",
    "| --------------------------------------------- | ------------------------------------------------------- | ------------------------------------------------------------------------- | -------------------------------------------------------------------------------- | -------------------------------------------------- | ---------------------------------------------------------------- |\n",
    "| **Standard $K$-fold**                         | `KFold(n_splits=K, shuffle=True)`                       | Any i.i.d. tabular data                                                   | Randomly partition into $K$ equal folds; hold one out each round                 | Simple, low variance                               | Folds may break stratification or group integrity                |\n",
    "| **Stratified $K$-fold**                       | `StratifiedKFold`                                       | Classification with class imbalance                                       | Keeps class proportions identical in every fold                                  | Fair evaluation of minority class                  | Not defined for regression                                       |\n",
    "| **Leave-One-Out (LOO)**                       | `LeaveOneOut()`                                         | Very small $n$ (≤ 100) or when bias must be minimal                       | Each observation is its own test set; $n$ fits                                   | Nearly unbiased; uses almost all data for training | High variance; $n$ times slower; not group-safe                  |\n",
    "| **Shuffle-Split / Monte-Carlo CV**            | `ShuffleSplit(n_splits=50, test_size=0.2)`              | Large datasets where full $K$-fold is costly                              | Randomly sample train/test partitions many times                                 | Flexible split sizes; good variance-bias trade-off | Overlaps across splits ⇒ scores not independent                  |\n",
    "| **Nested CV**                                 | `cross_val_score(GridSearchCV(...), X, y, cv=outer_cv)` | Hyper-parameter tuning when an unbiased generalisation estimate is needed | Outer loop estimates performance; inner loop tunes                               | Guards against “double dipping”                    | Computationally expensive                                        |\n",
    "| **Group $K$-fold**                            | `GroupKFold(n_splits=K)`                                | Data clustered by patient, user, site, etc.                               | Ensures all samples of a group stay together in train or test                    | Prevents leakage across correlated rows            | Needs a reliable group label                                     |\n",
    "| **Leave-One-Group-Out (LOGO)**                | `LeaveOneGroupOut()`                                    | Small # of groups (e.g., leave one patient out)                           | Each group becomes the lone test set in turn                                     | Max training data under group constraint           | High runtime if many groups                                      |\n",
    "| **Blocked / Rolling-Origin (Time-Series CV)** | `TimeSeriesSplit(n_splits=K, gap=∆)`                    | Temporal or sequential data (finance, sensor, NLP)                        | Preserve order: training window precedes test window; can expand or slide        | Mimics real-time forecasting; no look-ahead bias   | Old data may be less relevant; fewer training pts in early folds |\n",
    "| **Purged & Embargoed Split** (quant-finance)  | custom                                                  | High-frequency trades with overlapping labels                             | Remove (“purge”) overlapping events; embargo neighbours around test period       | Strictly avoids label leakage                      | Complex; reduces usable data                                     |\n",
    "| **Cluster-based CV**                          | `GroupKFold` with cluster ID                            | Spatial or network data; clustering validation                            | First cluster the data (or use known clusters) and treat each cluster as a group | Tests generalisation to unseen clusters            | Performance depends on clustering quality                        |\n",
    "| **Stratified‐Group K-fold**                   | `StratifiedGroupKFold` (scikit-learn ≥ 1.3)             | Imbalanced classes **and** groups                                         | Simultaneously stratifies by label and respects group boundaries                 | Fair & leakage-free                                | May be impossible if constraints clash                           |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
