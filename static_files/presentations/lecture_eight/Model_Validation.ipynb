{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec6a6a5d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a360912",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap\n",
    "\n",
    "We have discussed \n",
    "\n",
    "- Linear regression, polynomial regression, lasso regression, \n",
    "- kNN, loess, spline regression\n",
    "- Decision trees, boosting, random forests \n",
    "- Hierarchical clustering, K-means\n",
    "- PCA, tSNE \n",
    "\n",
    "Today: How to choose the *best* model (or to tune the hyperparameters)?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff8a0aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model validation\n",
    "\n",
    "- [Other names](https://en.wikipedia.org/wiki/Statistical_model_validation): model evaluation, model criticism\n",
    "- Goal: evaluate how well a model describe a dataset\n",
    "- Criteria (to convince others and *ourselves* our model makes sense)\n",
    "    - Subjective criteria: domain knowledge, common practice, interpretability\n",
    "    - **Objective/quantitative criteria**: numeric metrics \n",
    "- Methods (to ensure the criteria are evaluated properly)\n",
    "    - Validation with new data\n",
    "    - Validation with existing data\n",
    "        - **Split-sample validation, cross-validation**\n",
    "        - Residual analysis (e.g., residual plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053c1c37",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluation metrics\n",
    "\n",
    "We will take a brief glimpse at the metrics to use for \n",
    "\n",
    "- <span style=\"color:pink\">Supervised learning</span> with <span style=\"color:orange\">numeric</span> output\n",
    "- <span style=\"color:pink\">Supervised learning</span> with <span style=\"color:darkgreen\">categorical</span> output\n",
    "- <span style=\"color:lightblue\">Unsupervised learning</span> with <span style=\"color:orange\">numeric</span> output\n",
    "- <span style=\"color:lightblue\">Unsupervised learning</span> with <span style=\"color:darkgreen\">categorical</span> output\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97de1b30",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Supervised Learning with Numeric Output\n",
    "\n",
    "\n",
    "| Metric / loss                                     | What it measures                                  | When to use                               | Pros / Cons                                          |                      \n",
    "| ------------------------------------------------- | ------------------------------------------------- | ----------------------------------------- | ---------------------------------------------------- | \n",
    "| MSE / RMSE    | Quadratic loss; heavily penalises large errors    | Default for least-squares, Gaussian noise | Smooth, differentiable; sensitive to outliers        |                            \n",
    "| MAE                                                                           | Mean Absolute error                                       | Heavy-tailed noise, median regression | Robust to outliers; but gradient is constant (harder for some optimisers) |\n",
<<<<<<< HEAD
    "| $R^2$ / Adjusted $R^2$             | Fraction of variance explained                    | Linear models, presentation               | Intuitive; can be inflated by heteroskedasticity     |                                                                                                 |\n",
=======
    "| MAPE / SMAPE                                | Relative % error                                  | Retail demand, finance                    | Scale-free; explodes near 0                          |                                                                                                   |\n",
    "| $R^2$ / Adj-$R^2$             | Fraction of variance explained                    | Linear models, presentation               | Intuitive; can be inflated by heteroskedasticity     |                                                                                                 |\n",
>>>>>>> e9b90eecfe3600a52fb1b57c2e78cc89dc7318d6
    "| Log-likelihood                   | Average log-density under predictive distribution | Probabilistic regression, Bayesian models | Proper scoring rule; needs predictive density        |                                                                                                       |                                   \n",
    "| Rank-based metrics | Order rather than value                           | Recommenders, credit risk                 | Invariant to monotone transforms                     |                         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3820f9e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised Learning with Categorical Output\n",
    "\n",
    "| Metric / loss                                                | What it measures                     | When to use                               | Pros / Cons                                        |\n",
    "| ------------------------------------------------------------ | ------------------------------------ | ----------------------------------------- | -------------------------------------------------- |\n",
    "| Accuracy / Misclassicification rate                                    | Proportion of correct classifications | Balanced classes, quick sanity check      | Ignores class imbalance, probability calibration   |\n",
    "| [Precision / Recall](https://mlu-explain.github.io/precision-recall/)                                 | Confusion-matrix slices              | Imbalanced classes, information retrieval | Focus on positives       |\n",
    "| Specificity, Sensitivity                               | True-neg / true-pos rates            | Medical screening                         | Complements recall                                 |\n",
<<<<<<< HEAD
    "| [AUC-ROC](https://mlu-explain.github.io/roc-auc/)                      | Ranking quality as threshold varies  | Class-imbalance, ranking tasks            | Threshold-free |\n",
    "| Likelihood ratio                             | Model comparison (nested GLMs)       | Classic stats                             | Links to $\\chi^2$-square tests                          |\n"
=======
    "| [AUC-ROC](https://mlu-explain.github.io/roc-auc/)                      | Ranking quality as threshold varies  | Class-imbalance, ranking tasks            | Threshold-free; AU-PR preferred for rare positives |\n",
    "| Likelihood ratio / Deviance                              | Model comparison (nested GLMs)       | Classic stats                             | Links to chi-square tests                          |\n"
>>>>>>> e9b90eecfe3600a52fb1b57c2e78cc89dc7318d6
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a619a76",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [Confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix)\n",
    "\n",
    "|                     | Predicted Positive | Predicted Negative |\n",
    "|---------------------|--------------------|--------------------|\n",
    "| **Actual Positive**    | True Positive (TP) | False Negative (FN)|\n",
    "| **Actual Negative**     | False Positive (FP)| True Negative (TN) |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b95a73f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Unsupervised Learning with Numeric Output\n",
    "\n",
    "| Metric                                             | What / How                                                                        | Typical use-case                    | Pros / Cons                                                |\n",
    "| -------------------------------------------------- | --------------------------------------------------------------------------------- | ----------------------------------- | ---------------------------------------------------------- |\n",
<<<<<<< HEAD
    "| Within-cluster SSE (Inertia)             | Sum of squared errors        | K-means elbow method  | Fast; decreases monotonically—needs elbow or gap statistic |\n",
    "| Silhouette coefficient                        | internal vs. nearest-other-cluster distance | Any clustering with distance metric | Needs pairwise distances            |\n",
    "| Reconstruction error (MSE, MAE)                | PCA, auto-encoder                                         | Dim. reduction, anomaly detection   | Comparable to supervised losses                            |\n",
    "| Explained variance ratio                      | Variance captured by first $d$ PCs                                                  | PCA                                 | Easy to interpret                            |\n",
    "| Log-likelihood                       | Density fit of GMM, KDE, normalizing flows                                        | Model selection                     | Penalises over-fit by param count                          |\n"
=======
    "| Within-cluster SSE (Inertia, WCSS)             | Sum of squared errors        | K-means elbow method  | Fast; decreases monotonically—needs elbow or gap statistic |\n",
    "| Silhouette coefficient                        | internal vs. nearest-other-cluster distance | Any clustering with distance metric | Needs pairwise distances            |\n",
    "| Reconstruction error (MSE, MAE)                | PCA, auto-encoder                                         | Dim. reduction, anomaly detection   | Comparable to supervised losses                            |\n",
    "| Explained variance ratio                      | Variance captured by first $d$ PCs                                                  | PCA                                 | Easy to interpret; linear only                             |\n",
    "| Log-likelihood, AIC/BIC                       | Density fit of GMM, KDE, normalizing flows                                        | Model selection                     | Penalises over-fit by param count                          |\n"
>>>>>>> e9b90eecfe3600a52fb1b57c2e78cc89dc7318d6
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e15eba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unsupervised Learning with Categorical Output\n",
    "\n",
    "| Metric                                              | What / How                                                                     | Typical use-case                                        | Pros / Cons                                            |\n",
    "| --------------------------------------------------- | ------------------------------------------------------------------------------ | ------------------------------------------------------- | ------------------------------------------------------ |\n",
    "| Purity & Entropy                               | For each cluster, majority-class proportion / entropy; average across clusters | Quick external check when ground-truth labels available | Simple; ignores cluster size balance                   |\n",
    "| Adjusted Rand Index (ARI)                       | Pair-wise agreement corrected for chance                                       | External cluster validation                             | Sensitive to the number of clusters                       |\n",
    "| Normalized Mutual Information (NMI)             | MI between cluster labels & truth           | External validation (large K)                           | Symmetric; handles many classes                        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ee645c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Thoughts?\n",
    "\n",
    "- With so many metrics, should we consider evaluation measures of model evaluation measures?\n",
    "- How do we evaluate complex output? (1) Code. (2) Image. (3) Text.\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2395173",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Validation Methods\n",
    "\n",
    "Recall that \n",
    "- Validation with new data\n",
    "- Validation with existing data\n",
    "    - **Split-sample validation, cross-validation**\n",
    "    - Residual analysis (e.g., residual plots)\n",
    "\n",
    "Next: why do we need to employ proper validation methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864b42b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Complexity \n",
    "\n",
    "- Complexity of a model $\\approx$ number of parameters in the model\n",
    "    - More parameters $\\rightarrow$ more flexible to learn an unknown mechanism\n",
    "- Too many paramters $\\rightarrow$ the model learns the (noninformative, irreproducible, ungeneralizable) patterns of noise\n",
    "- [Numeric example with polynomial regression](vscode://file//Users/shizhe/Documents/Github/Website(Public)/Bias_variance.ipynb)\n",
    "- [Illustration by MLU](https://mlu-explain.github.io/bias-variance/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc69de84",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Double Descent\n",
    "\n",
    "- Occurs when model capacity grows beyond the point where it can *exactly* interpolate the training data\n",
    "- Highly over-parameterised models (deep neural nets, large ensembles, high-degree kernels) often defies the U-shape curve\n",
    "- Double-dip (hence double descent) phenomon \n",
    "- [Illustration by MLU](https://mlu-explain.github.io/double-descent/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f4babe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Validation Methods \n",
    "\n",
    "- Observation: error on new dataset explodes when model is too complex\n",
    "    \n",
    "    Solution: penalize model complexity \n",
    "\n",
    "- Information Criteria:\n",
<<<<<<< HEAD
    "    - [Akaike Information Criterion](https://en.wikipedia.org/wiki/Akaike_information_criterion)\n",
    "    $$ {\\rm AIC}= - 2 \\log (\\hat{L}) +2 k$$\n",
    "    - [Bayesian Information Criterion](https://en.wikipedia.org/wiki/Bayesian_information_criterion)\n",
    "    $$ {\\rm BIC}= - 2 \\log (\\hat{L}) +2 k \\log(n)$$"
=======
    "    - Akaike Information Criterion \n",
    "    - Bayesian Information Criterion "
>>>>>>> e9b90eecfe3600a52fb1b57c2e78cc89dc7318d6
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0283263",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Validation Methods \n",
    "\n",
    "- Observation: Validation with new data is the gold standard, but infeasible in practice  \n",
    "    \n",
    "    Solution: create \"new\" data from existing data\n",
    "\n",
    "- Validation with existing data using data spliting\n",
    "    - [Split-sample validation](vscode://file//Users/shizhe/Documents/Github/Website(Public)/Split_sample_validation.ipynb)\n",
    "    - [Cross-validation](vscode://file//Users/shizhe/Documents/Github/Website(Public)/Cross_validation.ipynb) (resampling method)\n",
    "    - [Bootstrap out-of-bag errors](vscode://file//Users/shizhe/Documents/Github/Website(Public)/OOB.ipynb) (resampling method, too)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40c90a7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
