{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e5822d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks II  \n",
    "## Data Structures "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d2ce88",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Structures of Data in Real World\n",
    "\n",
    "Data in real world often have structures\n",
    "\n",
    "- Continuity in space\n",
    "- Continuity in time  \n",
    "- Causal (and noncausal) relationships\n",
    "- ... \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e14378",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dismentling Structures: Restoring Independence for Inference\n",
    "\n",
    "Structures in data often lead to dependency\n",
    "\n",
    "Most statistical inference require independence \n",
    "\n",
    "Sometimes we have to dismentle the structures for proper inference \n",
    "- Blocking or stratification\n",
    "- Washout periods between experiments\n",
    "- Randomized controlled trials \n",
    "- ... \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047d79b2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Harnessing Structure: Architectures that Generalize\n",
    "\n",
    "Sometimes knowing the structures help\n",
    "\n",
    "- Image classification (convolutional neural networks)\n",
    "- Language translation (attention and transformer)\n",
    "- Social network anslysis (graph neural networks)\n",
    "- ... \n",
    "\n",
    "Main message for today: Architectures that encode prior knowledge need *fewer* parameters and generalize better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393fb34f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolution Neural Networks\n",
    "\n",
    "### Example: Image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a459226",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Structures of Images \n",
    "\n",
    "- Local continuity for most pixels \n",
    "- Objects/parts are more important than single pixel for prediction\n",
    "- ... \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c598c5e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Images in Computers' Eyes\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./figures/7pixel.png\" width=\"60%\" />\n",
    "</p>\n",
    "\n",
    "- Input: $\\mathbf{X} \\in \\mathbb{R}^{m\\times m}$ \n",
    "    - $x_{i,j}$: integer between 0 and 256\n",
    "    - $\\mathbf{X} \\in \\mathbb{R}^{m\\times m \\times 3}$ for colored image (RGB)\n",
    "\n",
    "- Goal: Classify the image into digits $(0-9)$\n",
    "\n",
    "\n",
    "\n",
    "Images from the [Modified National Institute of Standards and Technology database\n",
    "(MNIST) dataset](https://en.wikipedia.org/wiki/MNIST_database)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b0e70f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutional Neural Networks  \n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./figures/CNN.png\" width=\"70%\" />\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9eda85",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Convolution Layer\n",
    "\n",
    "Recall that a layer of MLP takes the form  \n",
    "$$ \\mathbf{z}^{(l)} = \\phi\\left( \\mathbf{W}^{(l)}\\,\\mathbf{z}^{(l-1)} + \\mathbf{b}^{(l)} \\right) , $$\n",
    "\n",
    "\n",
    "A convolution layer replaces the big matrix multiplication $\\mathbf{W}^{(l)} \\in \\mathbb{R}^{p \\times m}$ with a *convolution*, for each $i,j$,\n",
    "\n",
    "$$z_{ij}^{(1)}= \\sum_{u=1}^k \\sum_{v=1}^k w_{uv}^{(k,c)}\\;x_{i+u,j+v}^{c}+b^{k}$$\n",
    "\n",
    "- Local *receptive field*: filter size $k \\times k$ (e.g. $3\\times 3$) looks only at nearby pixels.  \n",
    "- Weight sharing: the same kernel slides across the image \n",
    "- Usually followed by a ReLU activation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1c4a4c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolution \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./figures/convo_layer.png\" width=\"70%\" />\n",
    "</p>\n",
    "\n",
    "* Input dimension $M$ (often **W**idth or **H**eight)\n",
    "* Kernel/filter size $K$ (typically 3 or 5)\n",
    "* Stride $S$\n",
    "* Padding $P$\n",
    "* Output dimension $(M+2P-K)/S +1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6f2b3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolution Layer: Example \n",
    "<p align=\"center\">\n",
    "  <img src=\"./figures/7convo.png\" width=\"80%\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7874ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pooling Layer  \n",
    "\n",
    "Pooling: Down-sample while keeping the most salient signal.\n",
    "\n",
    "- Max-Pool: $z_{ij}=\\max_{u,v} x_{(i+u)(j+v)}$   \n",
    "    (translation/shift invariance)\n",
    "- Average-Pool  $z_{ij}=k^{-2}\\sum_{u,v} x$   \n",
    "    (smooths signals)\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./figures/max_pool.png\" width=\"60%\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a96f83",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [Convolutional Neural Networks: Example](vscode://file//Users/shizhe/Documents/Github/Website(Public)/CNN.ipynb)\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./figures/7cnn.png\" width=\"80%\" />\n",
    "</p>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af59cc8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CNN v.s. FNN\n",
    "\n",
    "In the [example](vscode://file//Users/shizhe/Documents/Github/Website(Public)/CNN.ipynb), you will find \n",
    "\n",
    "1. A convolutional neural network\n",
    "```python\n",
    "    self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                 # 14×14\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2)                  # 7×7\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*7*7, 128), nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "```\n",
    "2. An FNN/MLP \n",
    "```python\n",
    "    self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(784, 512), nn.ReLU(),\n",
    "            nn.Linear(512,  32), nn.ReLU(),\n",
    "            nn.Linear( 32,  10)\n",
    "        )\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3b5837",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Performance\n",
    "\n",
    "<section>\n",
    "\n",
    "<table style=\"width: 100%;border: none; border-collapse: collapse;\">\n",
    "<tr>\n",
    "\n",
    "<!-- Text Column -->\n",
    "<td style=\"width: 50%; vertical-align: top; padding-right: 20px;border: none;\">\n",
    "\n",
    "- Number of parameters (trainable weights)\n",
    "    - CNN: 421,642\n",
    "    - FNN: 418,666\n",
    "\n",
    "- Train losses look fine for both models\n",
    "\n",
    "- Misclassification on test sets:\n",
    "    - CNN: $18.1\\%$\n",
    "    - FNN: $47.1\\%$\n",
    "</td>\n",
    "\n",
    "<!-- Image Column -->\n",
    "<td style=\"width: 50%;border: none;\">\n",
    "  <img src=\"./figures/loss_cnn_fnn.png\" alt=\"attention\" style=\"max-width: 100%;\">\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "</section>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2c28a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Convolutional Neural Networks: Milestones\n",
    "\n",
    "\n",
    "| Year | Architecture | # Layers |\n",
    "|------|--------------|-----------|\n",
    "| 1998 | LeNet‑5 | 5 |\n",
    "| 2012 | AlexNet |8 |\n",
    "| 2014 | VGG | 19 |\n",
    "| 2015 | ResNet | **152** |\n",
    "| 2017| SENet | **152** |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db48e67",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Key to Go Deep: ResNet (He et al. 2015)\n",
    "\n",
    "\n",
    "<section>\n",
    "  <div style=\"display: flex; align-items: center;\">\n",
    "    <!-- Left: Text -->\n",
    "    <div style=\"flex: 3; padding-right: 20px;\">\n",
    "      \n",
    "- Prior to ResNet: deeper models yield larger training error\n",
    "\n",
    "- Contradictory to known facts:\n",
    "    - Deeper models can capture more complex mappings\n",
    "    - CNNs already pass the interpolation threshold (second descent) (AlexNet has around 60 million parameters)\n",
    "\n",
    "- Hypothesis: large training error is a result of failures in optimization \n",
    "    - Possible cause: vanishing gradients in backpropogation \n",
    "\n",
    "- Solution: explicitly include an identity mapping to let gradient flows   \n",
    "    Desired mapping $H(x) $\n",
    "    \"Residual\"/nonlinear mapping $F(x)$\n",
    "\n",
    "    </div>\n",
    "\n",
    "    <!-- Right: Image -->\n",
    "    <div style=\"flex: 2;\">\n",
    "    <img src=\"./figures/resnet_orig.png\" alt=\"\" width=\"300\" align=\"center\">\n",
    "    </div>\n",
    "  </div>\n",
    "</section>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59579b70",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Attention and Transformer\n",
    "\n",
    "### Example: Translation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79125611",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine Translation Examples\n",
    "\n",
    "\n",
    "French to English  \n",
    "\n",
    "- **FR:** <span style=\"color:#d62728\">Demain</span>, <span style=\"color:#1f77b4\">je</span> <span style=\"color:#2ca02c\">donnerai</span> <span style=\"color:#9467bd\">le&nbsp;livre</span> <span style=\"color:#ff7f0e\">à&nbsp;mon&nbsp;ami</span>.  \n",
    "- **EN:** <span style=\"color:#1f77b4\">I</span> <span style=\"color:#2ca02c\">will&nbsp;give</span> <span style=\"color:#9467bd\">the&nbsp;book</span> <span style=\"color:#ff7f0e\">to&nbsp;my&nbsp;friend</span> <span style=\"color:#d62728\">tomorrow</span>.\n",
    "\n",
    "Spanish to English  \n",
    "\n",
    "- **ES:** <span style=\"color:#d62728\">Ayer</span>, <span style=\"color:#1f77b4\">María</span> <span style=\"color:#2ca02c\">le&nbsp;envió</span> <span style=\"color:#9467bd\">una&nbsp;carta</span> <span style=\"color:#ff7f0e\">a&nbsp;su&nbsp;hermano</span> <span style=\"color:#8c564b\">desde&nbsp;México</span>.  \n",
    "- **EN:** <span style=\"color:#1f77b4\">Maria</span> <span style=\"color:#2ca02c\">sent</span> <span style=\"color:#ff7f0e\">her&nbsp;brother</span> <span style=\"color:#9467bd\">a&nbsp;letter</span> <span style=\"color:#8c564b\">from&nbsp;Mexico</span> <span style=\"color:#d62728\">yesterday</span>.\n",
    "\n",
    "Chinese to English  \n",
    "\n",
    "- **ZH:** <span style=\"color:#1f77b4\">我</span><span style=\"color:#8c564b\">在图书馆</span><span style=\"color:#2ca02c\">借了</span><span style=\"color:#9467bd\">一本有趣的书</span>。  \n",
    "- **EN:** <span style=\"color:#1f77b4\">I</span> <span style=\"color:#2ca02c\">borrowed</span> <span style=\"color:#9467bd\">an&nbsp;interesting&nbsp;book</span> <span style=\"color:#8c564b\">from&nbsp;the&nbsp;library</span>.\n",
    "\n",
    "\n",
    "Text in the eyes of computers: [Tokenizer playground](https://huggingface.co/spaces/Xenova/the-tokenizer-playground)\n",
    "\n",
    "\n",
    "[French numbers to English using Transformer](vscode://file//Users/shizhe/Documents/Github/Website(Public)/Transformer_Number.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaf69d1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "## Sequence-to-Sequence Task\n",
    "\n",
    "- Machine Translation (e.g., French to English sentence translation)\n",
    "- Music Generation (input: musical themes or motifs, output: extended music sequences)\n",
    "- Code Generation from Natural Language (input: problem description, output: source code)\n",
    "- ...\n",
    "\n",
    "Key idea: **Encode** the input into a context state $c$ then **decode** step‑by‑step with RNN/Transformer.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./figures/encoder_decoder_marked.png\" width=\"70%\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771d6d99",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recurrent Neural Networks  \n",
    "\n",
    "Encoder \n",
    "$$\n",
    "\\mathbf{h}_t = f\\bigl( \\mathbf{W}_{hx}\\mathbf{x}_t + \\mathbf{W}_{hh}\\mathbf{h}_{t-1} + \\mathbf{b} \\bigr),\\qquad\n",
    "$$\n",
    "\n",
    "Decoder\n",
    "$$\\mathbf{s}_t = g\\big(\\mathbf{U}_{sc}\\mathbf{c} + \\mathbf{U}_{sy}\\mathbf{y}_{t-1}+\\mathbf{U}_{ss}\\mathbf{s}_{t-1}\\big)$$\n",
    "\n",
    "- Shares parameters across all time steps \n",
    "- Trained with *Back‑Propagation Through Time* (BPTT).  \n",
    "- Suffers from gradient vanishing for long $T$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5fd14d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "Encoder\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./figures/encoder_color.png\" width=\"70%\" />\n",
    "</p>\n",
    "\n",
    "\n",
    "Decoder\n",
    "<p align=\"center\">\n",
    "  <img src=\"./figures/decoder_color.png\" width=\"70%\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31ceb7a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem with the Context \n",
    "\n",
    "One problem with using RNN is that \n",
    "\n",
    "- All information of the inputs are stored in one context vector $c$. \n",
    "- All decodings are based on $c$\n",
    "\n",
    "This could have a few \n",
    "- Might fail for long sequences \n",
    "- Might not be optimal for sequential decoding\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./figures/encoder_decoder_compact.png\" width=\"70%\" />\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850be204",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Attention: Step 1\n",
    "\n",
    "<section>\n",
    "\n",
    "<table style=\"width: 100%;border: none; border-collapse: collapse;\">\n",
    "<tr>\n",
    "\n",
    "<!-- Text Column -->\n",
    "<td style=\"width: 45%; vertical-align: top; padding-right: 20px;border: none;\">\n",
    "\n",
    "\n",
    "  1. Compute alignment scores $e_{1,i} \\in \\mathbb{R}$\n",
    "  $$e_{1,i}=f_{\\rm att}(s_{0},h_i)$$\n",
    "\n",
    "  2. Apply softmax to obtain attention weights (weights sum up to 1)\n",
    "  $$ a_{1,i}= \\frac{\\exp(e_{1,i})}{\\sum_{j=1}^{3} \\exp(e_{1,j})} $$\n",
    "\n",
    "  3. Compute context vector as the weighted sum of hidden states\n",
    "  $$c_1 = \\sum_{i=1}^3 a_{1,i}h_i$$\n",
    "\n",
    "  4. Decode the new state \n",
    "  $$s_1 = g\\big(\\mathbf{U}_{sc}c_1 + \\mathbf{U}_{sy}{y}_{0}+\\mathbf{U}_{ss}{s}_{0}\\big)$$\n",
    "\n",
    "</td>\n",
    "\n",
    "<!-- Image Column -->\n",
    "<td style=\"width: 55%;border: none;\">\n",
    "  <img src=\"./figures/attention_step_1.png\" alt=\"attention\" style=\"max-width: 100%;\">\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "</section>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f33c14",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Attention: Step $t$\n",
    "\n",
    "<section>\n",
    "\n",
    "<table style=\"width: 100%;border: none; border-collapse: collapse;\">\n",
    "<tr>\n",
    "\n",
    "<!-- Text Column -->\n",
    "<td style=\"width: 45%; vertical-align: top; padding-right: 20px;border: none;\">\n",
    "\n",
    "\n",
    "1. Compute alignment scores $e_{t,i} \\in \\mathbb{R}$\n",
    "$$e_{t,i}=f_{\\rm att}(s_{t-1},h_i)$$\n",
    "\n",
    "2. Apply softmax to obtain attention weights (weights sum up to 1)\n",
    "$$ a_{t,i}= \\frac{\\exp(e_{t,i})}{\\sum_{j=1}^{3} \\exp(e_{t,j})} $$\n",
    "\n",
    "3. Compute context vector as the weighted sum of hidden states\n",
    "$$c_t = \\sum_{i=1}^3 a_{t,i}h_i$$\n",
    "\n",
    "4. Decode the new state \n",
    "$$s_t = g\\big(\\mathbf{U}_{sc}c_t + \\mathbf{U}_{sy}{y}_{t-1}+\\mathbf{U}_{ss}{s}_{t-1}\\big)$$\n",
    "\n",
    "</td>\n",
    "\n",
    "<!-- Image Column -->\n",
    "<td style=\"width: 55%;border: none;\">\n",
    "  <img src=\"./figures/attention_step_2.png\" alt=\"attention\" style=\"max-width: 100%;\">\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "</section>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3880c0c9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Attention: Example \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./figures/attention_example.png\" width=\"50%\" />\n",
    "</p>\n",
    "\n",
    "\n",
    "Source: Figure 2(a) from [Bahdanau et al. (2015)](https://arxiv.org/abs/1409.0473)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b667e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Self-Attention\n",
    "\n",
    "<section>\n",
    "\n",
    "<table style=\"width: 100%;border: none; border-collapse: collapse;\">\n",
    "<tr>\n",
    "\n",
    "<!-- Text Column -->\n",
    "<td style=\"width: 50%; vertical-align: top; padding-right: 20px;border: none;\">\n",
    "\n",
    "Observation: Hidden states and decoding states are both functions of input $\\mathbf{X}$\n",
    "\n",
    "\n",
    "Self-Attention: new architectures based on the input $\\mathbf{X}$\n",
    "$$\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt d}\\right)V,$$\n",
    "where the query vector $Q$, key vector $K$, and value vector $V$ are all functions of input $\\mathbf{X}$\n",
    "\n",
    "For more detail, see [Lecture 8](https://cs231n.stanford.edu/slides/2024/lecture_4.pdf) of Stanford CS231n by Fei-Fei Li et al.\n",
    "\n",
    "\n",
    "\n",
    "</td>\n",
    "\n",
    "<!-- Image Column -->\n",
    "<td style=\"width: 50%;border: none;\">\n",
    "  <img src=\"./figures/self_attention.png\" alt=\"attention\" style=\"max-width: 100%;\">\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "</section>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd60e76c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transformer \n",
    "\n",
    "<section>\n",
    "\n",
    "<table style=\"width: 100%;border: none; border-collapse: collapse;\">\n",
    "<tr>\n",
    "\n",
    "<!-- Text Column -->\n",
    "<td style=\"width: 50%; vertical-align: top; padding-right: 20px;border: none;\">\n",
    "\n",
    "Stacking multi‑head self‑attention layers yields the **Transformer** (Vaswani et al., 2017). \n",
    "\n",
    "Key advantages:\n",
    "\n",
    "- Parallelizable (vs. sequential RNN).\n",
    "- Captures long‑range dependencies.\n",
    "- Scales with data \n",
    "\n",
    "</td>\n",
    "\n",
    "<!-- Image Column -->\n",
    "<td style=\"width: 50%;border: none;\">\n",
    "  <img src=\"./figures/transformer.png\" alt=\"attention\" style=\"max-width: 100%;\">\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "</section>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e687316",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transformer  \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./figures/transformer_paper.png\" width=\"50%\" />\n",
    "</p>\n",
    "\n",
    "Interactive demo: [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7290f1a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "Real‑world data come with *structure*:\n",
    "\n",
    "* **Grid‑like** → images (2‑D pixels), audio spectrograms (1‑D or 2‑D).\n",
    "* **Sequential** → text, time‑series, DNA.\n",
    "\n",
    "Neural layers that *leverage* structures learn faster and generalize better:\n",
    "\n",
    "| Data type | Good default layer | Key idea |\n",
    "|-----------|-------------------|----------|\n",
    "| Grid      | Convolution       | local receptive field & weight sharing |\n",
    "| Sequence  | Recurrent / Transformer | hidden state or attention over positions |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
